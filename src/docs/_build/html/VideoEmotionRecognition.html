<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>VideoEmotionRecognition Documentation &mdash; viemr 0.2.8 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=9265798b"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="&lt;no title&gt;" href="modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            viemr
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">VideoEmotionRecognition Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.add_emotions"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.add_emotions()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.audio"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.audio()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.emotion_recognition"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.emotion_recognition()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.get_heatmap"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.get_heatmap()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.get_labels"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.get_labels()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.set_vtt"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.set_vtt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.text"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.text()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.transcript"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.transcript()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.video"><code class="docutils literal notranslate"><span class="pre">VideoEmotionRecognition.video()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">viemr</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">&lt;no title&gt;</a></li>
      <li class="breadcrumb-item active">VideoEmotionRecognition Documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/VideoEmotionRecognition.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-VideoEmotionRecognition.viemr">
<span id="videoemotionrecognition-documentation"></span><h1>VideoEmotionRecognition Documentation<a class="headerlink" href="#module-VideoEmotionRecognition.viemr" title="Permalink to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">VideoEmotionRecognition.viemr.</span></span><span class="sig-name descname"><span class="pre">VideoEmotionRecognition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mp4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.add_emotions">
<span class="sig-name descname"><span class="pre">add_emotions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_emotions</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.add_emotions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.add_emotions" title="Permalink to this definition"></a></dt>
<dd><p>Use this to add the arousal and valence of a new emotion by passing a series or a dataframe with it’s name, x coordinate
and y coordinate in this order. Useful when changing the model so that the emotions classified by this model are recognized.</p>
<dl class="simple">
<dt>Args:</dt><dd><p><strong>new_emotions (pandas series/dataframe)</strong>: new emotion to be added with it’s name, arousal and valence coordinates values.</p>
</dd>
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.audio">
<span class="sig-name descname"><span class="pre">audio</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Rajaram1996/Hubert_emotion'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.audio"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.audio" title="Permalink to this definition"></a></dt>
<dd><p>One of the main functions of this module. It’s responsible for classifying
the emotions present in the sentences of a clip based on it’s audio.</p>
<p>Can be used manually or indirectly via the emotion_recogniton(modality = “audio”) method to generate a dataframe
with each sentence of a video transcripted with it’s timestamps, the emotion that is predominant in it’s audio 
segment and the score of the emotion atributed by the model.</p>
<dl>
<dt>Args:</dt><dd><p><strong>method</strong> : placeholder for future integration of new models.</p>
<p><strong>redo (bool)</strong>: This function saves if it was already used before, avoiding repeating the repeated work if, for example,
the multimodal classification is used but the text modality has already been done before. Setting this to True 
will force it to repeat the work.</p>
</dd>
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
<p>The current implementation is based on a Hubert model that can be found here <a class="reference external" href="https://github.com/m3hrdadfi/soxan">https://github.com/m3hrdadfi/soxan</a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.emotion_recognition">
<span class="sig-name descname"><span class="pre">emotion_recognition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modality</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text_pipeline</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text_encoder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">audio_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Rajaram1996/Hubert_emotion'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">video_frames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.emotion_recognition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.emotion_recognition" title="Permalink to this definition"></a></dt>
<dd><p>Main functionality that classifies the emotions present in a clip. The modality determines which aspect of the video
will be used to classify the emotions</p>
<dl>
<dt>Args:</dt><dd><p><strong>modality (string)</strong>: Choose which way you want the video to be analyzed between 4 possibilities
* transcript: classify the emotions based on the transcription of it’s audio
* audio: classify the emotions based on it’s audio
* video: classify the emotions based on the facial expressions present
* multimodal: classify the emotions using both the text and audio modalityes, which are integrated using a graph model.</p>
<p><strong>text_pipe (pipeline)</strong>: pass a pipeline in order to change the model being used to classify the emotions.</p>
<p><strong>text_encoder (encoder)</strong>: pass the encoder used in the pipeline passed.</p>
<p><strong>video_frames (int)</strong>: change the amount of frames per second going to be analized by the model. Bigger values can
improve the classification, but drastically increasing the time of execution.</p>
<p><strong>redo (bool)</strong>: This function saves if it was already used before, avoiding repeating the repeated work if, for example,
the multimodal classification is used but the text modality has already been done before. Setting this to True 
will force it to repeat the work.</p>
</dd>
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.get_heatmap">
<span class="sig-name descname"><span class="pre">get_heatmap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modality</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">animated</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">join_video</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.get_heatmap"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.get_heatmap" title="Permalink to this definition"></a></dt>
<dd><blockquote>
<div><p>Generates a heatmap of the evolution of emotions during the video. This function should be used after 
the emotion_recognition has been used at least once with any modality. Ithout any special flags, this method
generates an image of a heatmap with the overall emotions of the video, but with the right flags, this image
cn be instead a video demonstrating the evolution of the emotions with the passage of time and even integrate
this video with the original one from which the emotions were extracted.</p>
<dl>
<dt>Args:</dt><dd><p><strong>animated (bool)</strong> instead of a single image, generate a video showing the emotions during the timestamps,
which can be adjusted via the window and stride parameters.</p>
<p><strong>window (int)</strong>: size in seconds of the window shown in the animated heatmap. Only works if animated = True. 
This also changes the heatmap generate with join_video = True</p>
<p><strong>stride (int)</strong>: size in seconds of the difference between two consecutives timeframes. Only works if animated = True. 
This also changes the heatmap generate with join_video = True</p>
<p><strong>join_video (bool)</strong>: create a new video with the original video and the heatmap video side by side. Only works if animated = True.</p>
<p><strong>modality (string)</strong> Choose which way you want the video to be analyzed between 4 possibilities
* transcript: classify the emotions based on the transcription of it’s audio
* audio: classify the emotions based on it’s audio
* video: show the classified emotions based on the facial expressions present.
* multimodal: classify the emotions using both the text and audio modalityes, which are integrated using a graph model. 
* all: If either no modality argument is passed or the all modality is used, it will show all the modalities performed 
in it’s own columns(for example, text in text_label), while the label and prob column show the results of the modality 
that was selected the last time this method was called.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><p>Based on the flags, a plot of the heatmap, a video of the heatmap or a merge of the original video and the video of the heatmap.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.get_labels">
<span class="sig-name descname"><span class="pre">get_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modality</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.get_labels"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.get_labels" title="Permalink to this definition"></a></dt>
<dd><blockquote>
<div><p>Returns the dataframe with all sentences, classificated or not. With
modality, you can select to show only the classification for the
modality selected in case of multiple classifications. Doesn’t work
for multimodal since there is no classification.</p>
<p>The emotions and probability assigned by the model are shon respectively int the label and prob columns.</p>
<dl class="simple">
<dt>Args:</dt><dd><p><strong>modality (string)</strong> Choose which way you want the video to be analyzed between 4 possibilities
* transcript: classify the emotions based on the transcription of it’s audio
* audio: classify the emotions based on it’s audio
* video: show the classified emotions based on the facial expressions present. 
* all: If either no modality argument is passed or the all modality is used, it will show all the modalities performed 
in it’s own columns(for example, text in text_label), while the label and prob column show the results of the modality 
that was selected the last time this method was called.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.set_vtt">
<span class="sig-name descname"><span class="pre">set_vtt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.set_vtt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.set_vtt" title="Permalink to this definition"></a></dt>
<dd><blockquote>
<div><p>Alternate form to load a video dataframe via it’s vtt, which can be generated previously with the whisperx transcription.</p>
<dl class="simple">
<dt>Args:</dt><dd><p><strong>file (vtt)</strong>: path to the vtt file to be used.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.text">
<span class="sig-name descname"><span class="pre">text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">emot_pipe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_text</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.text"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.text" title="Permalink to this definition"></a></dt>
<dd><p>One of the main functions of this module. It’s responsible for classifying
the emotions present in a clip based on the transcription of it’s audio.</p>
<p>Can be used manually or indirectly via the emotion_recogniton(modality = “transcript”) method to generate a dataframe
with each sentence of a video transcripted with it’s timestamps, the emotion that is predominant in the text of this
sentence and the score of the emotion atributed by the model.</p>
<dl>
<dt>Args:</dt><dd><p><strong>emot_pipe (pipeline)</strong>: pass a pipeline in order to change the model being used to classify the emotions.</p>
<p><strong>encoder_text (encoder)</strong>: pass the encoder used in the pipeline passed.</p>
<p><strong>redo (bool)</strong>: This function saves if it was already used before, avoiding repeating the repeated work if, for example,
the multimodal classification is used but the text modality has already been done before. Setting this to True 
will force it to repeat the work.</p>
</dd>
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
<p>The current implementation is based on a Roberta model trained on goemotions, but since the goemotions database is in english
there is a pre processing of translating the text in portuguese to english with this Unicamp project <a class="reference external" href="https://huggingface.co/unicamp-dl/translation-pt-en-t5">https://huggingface.co/unicamp-dl/translation-pt-en-t5</a></p>
<p>You can also change the model being used for the emotion recognition with the emot_pipe and encoder_text arguments, 
although you may need to add the labels that are classified by the new model via the method add_emotions() 
if they are not present in the self.emotions_coord dataframe</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.transcript">
<span class="sig-name descname"><span class="pre">transcript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'whisperx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_time</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.transcript"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.transcript" title="Permalink to this definition"></a></dt>
<dd><p>One of the main functions of this module. It’s responsible for transforming the audio of a video into text,
organizing it’s sentences in a dataframe with the transcription and it’s time frames of start and end.</p>
<p>This function is used in all the modalities of classification, but doesn’t need to be run manually, 
being automatically used in each modality.</p>
<dl>
<dt>Args:</dt><dd><p><strong>method (string)</strong>: placeholder for future integration of new models.</p>
<p><strong>min_time (int)</strong>: minimun time in seconds for a sentence to be added into the dataframe. Increassing this value will discard shorter sentences</p>
</dd>
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="VideoEmotionRecognition.viemr.VideoEmotionRecognition.video">
<span class="sig-name descname"><span class="pre">video</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">frames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/VideoEmotionRecognition/viemr.html#VideoEmotionRecognition.video"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#VideoEmotionRecognition.viemr.VideoEmotionRecognition.video" title="Permalink to this definition"></a></dt>
<dd><p>One of the main functions of this module. It’s responsible for classifying
the emotions present in the sentences of a clip based on the facial expressions present.</p>
<p>Can be used manually or indirectly via the emotion_recogniton(modality = “video”) method to generate a dataframe
with each sentence of a video transcripted with it’s timestamps, the emotion that is predominant in the facial
expressions present and the score of the emotion atributed by the model.</p>
<dl>
<dt>Args:</dt><dd><blockquote>
<div><p><strong>frames (int)</strong>: change the amount of frames per second going to be analized by the model. Bigger values can
improve the classification, but drastically increasing the time of execution.</p>
<p><strong>redo (bool)</strong>: This function saves if it was already used before, avoiding repeating the repeated work if, for example,
the multimodal classification is used but the text modality has already been done before. Setting this to True 
will force it to repeat the work.</p>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><p>Nothing.</p>
</dd>
</dl>
<p>The current implementation is based on a Deepface, which can be found here <a class="reference external" href="https://github.com/serengil/deepface">https://github.com/serengil/deepface</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modules.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Artur.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>